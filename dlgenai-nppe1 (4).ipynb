{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":119874,"databundleVersionId":14372465,"sourceType":"competition"},{"sourceId":637036,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":480303,"modelId":495990},{"sourceId":637689,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":480721,"modelId":496355}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:15:21.894234Z","iopub.execute_input":"2025-11-08T10:15:21.894523Z","iopub.status.idle":"2025-11-08T10:16:12.078237Z","shell.execute_reply.started":"2025-11-08T10:15:21.894500Z","shell.execute_reply":"2025-11-08T10:16:12.077190Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"# !pip install trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T13:55:10.943516Z","iopub.execute_input":"2025-11-08T13:55:10.943909Z","iopub.status.idle":"2025-11-08T13:55:49.754979Z","shell.execute_reply.started":"2025-11-08T13:55:10.943875Z","shell.execute_reply":"2025-11-08T13:55:49.753944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # import numpy as np\n# import pandas as pd\n# import torch\n# # from torch import nn\n# from torch.utils.data import DataLoader, random_split\n# import torchmetrics\n# from torchvision import datasets, transforms\n# import trackio\n# import torch.nn as nn\n# from torchvision import models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T13:55:57.645185Z","iopub.execute_input":"2025-11-08T13:55:57.645466Z","iopub.status.idle":"2025-11-08T13:56:20.277859Z","shell.execute_reply.started":"2025-11-08T13:55:57.645441Z","shell.execute_reply":"2025-11-08T13:56:20.277086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T13:56:27.267460Z","iopub.execute_input":"2025-11-08T13:56:27.267861Z","iopub.status.idle":"2025-11-08T13:56:27.276466Z","shell.execute_reply.started":"2025-11-08T13:56:27.267834Z","shell.execute_reply":"2025-11-08T13:56:27.275417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Huggingface Access Adding token to environment variable\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# import os\n# os.environ['HF_TOKEN']  = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:00:47.400142Z","iopub.execute_input":"2025-11-10T04:00:47.400493Z","iopub.status.idle":"2025-11-10T04:00:47.491019Z","shell.execute_reply.started":"2025-11-10T04:00:47.400473Z","shell.execute_reply":"2025-11-10T04:00:47.490216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trackio.init(\n#     project=\"25-t3-nppe1\", \n#     space_id=\"ShreyaAgr/dlgenai-nppe\",\n#     name=\"cnn_baseline\",\n#     group=\"simple\"\n# )\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:20.802782Z","iopub.execute_input":"2025-11-08T10:17:20.803070Z","iopub.status.idle":"2025-11-08T10:17:20.808395Z","shell.execute_reply.started":"2025-11-08T10:17:20.803047Z","shell.execute_reply":"2025-11-08T10:17:20.807212Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Scratch Model","metadata":{}},{"cell_type":"markdown","source":"### Data loading","metadata":{}},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset, DataLoader\n# from torchvision import transforms\n# from PIL import Image\n# import pandas as pd\n# from pathlib import Path\n\n# # Paths\n# DATA_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset\")\n# IMG_DIR = DATA_DIR     # folder with all training images\n# CSV_PATH = DATA_DIR / \"train.csv\" # contains: image_name, gender, age\n\n# # Read CSV\n# df = pd.read_csv(CSV_PATH)\n# # print(\"Sample data:\\n\", df.head())\n\n# # Train-validation split\n# from sklearn.model_selection import train_test_split\n# train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"gender\"], random_state=42)\n\n# # Transforms\n# transform = transforms.Compose([\n#     transforms.Resize((128, 128)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n# ])\n\n# from torch.utils.data import Dataset\n# from PIL import Image\n# import torch\n# from pathlib import Path\n\n# class FaceDataset(Dataset):\n#     def __init__(self, dataframe, root_dir, transform=None):\n#         self.data = dataframe\n#         self.root_dir = Path(root_dir)\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.data)\n\n#     def __getitem__(self, idx):\n#         row = self.data.iloc[idx]\n#         img_path = self.root_dir / row[\"full_path\"]   #   full path like \"face_dataset/train/00001.jpg\"\n#         image = Image.open(img_path).convert(\"RGB\")\n\n#         gender = torch.tensor(row[\"gender\"], dtype=torch.float32)\n#         age = torch.tensor(row[\"age\"], dtype=torch.float32)\n\n#         if self.transform:\n#             image = self.transform(image)\n\n#         return image, gender, age\n\n\n# # Create datasets & loaders\n# train_dataset = FaceDataset(train_df, IMG_DIR, transform)\n# val_dataset   = FaceDataset(val_df, IMG_DIR, transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n# val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# # Device setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# print(f\" Train: {len(train_dataset)} | Val: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T13:56:46.740438Z","iopub.execute_input":"2025-11-08T13:56:46.741143Z","iopub.status.idle":"2025-11-08T13:56:46.803742Z","shell.execute_reply.started":"2025-11-08T13:56:46.741111Z","shell.execute_reply":"2025-11-08T13:56:46.802772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class AgeGenderCnn(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.features = nn.Sequential(\n#             nn.Conv2d(3, 32, 3, padding=1),\n#             nn.ReLU(),\n#             nn.BatchNorm2d(32),\n#             nn.MaxPool2d(2, 2),  # 128 -> 64\n\n#             nn.Conv2d(32, 64, 3, padding=1),\n#             nn.ReLU(),\n#             nn.BatchNorm2d(64),\n#             nn.MaxPool2d(2, 2)   # 64 -> 32\n#         )\n\n#         self.flatten = nn.Flatten()\n#         self.fc = nn.Sequential(\n#             nn.Linear(64 * 32 * 32, 128),  #   fixed here\n#             nn.ReLU(),\n#             nn.Dropout(0.4),\n#             nn.Linear(128, 64),\n#             nn.ReLU(),\n#             nn.Dropout(0.4)\n#         )\n\n#         self.age_head = nn.Linear(64, 1)\n#         self.gender_head = nn.Linear(64, 1)\n\n#     def forward(self, x):\n#         x = self.features(x)\n#         x = self.flatten(x)\n#         x = self.fc(x)\n#         age = self.age_head(x)\n#         gender = torch.sigmoid(self.gender_head(x))\n#         return age, gender","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:20.879591Z","iopub.execute_input":"2025-11-08T10:17:20.879940Z","iopub.status.idle":"2025-11-08T10:17:20.903741Z","shell.execute_reply.started":"2025-11-08T10:17:20.879913Z","shell.execute_reply":"2025-11-08T10:17:20.902308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = AgeGenderCnn().to(device)\n\n# # Separate losses\n# criterion_age = nn.MSELoss()           # regression\n# criterion_gender = nn.BCEWithLogitsLoss()  # binary classification\n\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:20.904812Z","iopub.execute_input":"2025-11-08T10:17:20.905230Z","iopub.status.idle":"2025-11-08T10:17:20.923434Z","shell.execute_reply.started":"2025-11-08T10:17:20.905200Z","shell.execute_reply":"2025-11-08T10:17:20.921930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# epochs = 3\n# loss_history = []\n\n# for epoch in range(epochs):\n#     model.train()\n#     total_loss = 0\n\n#     for xb, gender_target, age_target in train_loader:  # ensure your dataset returns both\n#         xb, age_target, gender_target = xb.to(device), age_target.to(device), gender_target.to(device)\n\n#         pred_age, pred_gender = model(xb)\n\n#         loss_age = criterion_age(pred_age.squeeze(1), age_target.float())\n#         loss_gender = criterion_gender(pred_gender.squeeze(1), gender_target.float())\n#         loss = loss_age + loss_gender\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         total_loss += loss.item()\n\n#     avg_loss = total_loss / len(train_loader)\n#     # print(\"iter \",i,\"loss \",avg_loss)\n#     loss_history.append(avg_loss)\n\n#     print(f\"Epoch [{epoch+1}/{epochs}] | Loss: {avg_loss:.4f}\")\n\n#     # Log training progress to TrackIO\n#     trackio.log({\"train/loss\": avg_loss, \"train/loss_age\": loss_age.item(), \"train/loss_gender\": loss_gender.item(), \"epoch\": epoch})\n\n# trackio.finish()\n# print(\" Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:20.924582Z","iopub.execute_input":"2025-11-08T10:17:20.924851Z","iopub.status.idle":"2025-11-08T10:17:20.948666Z","shell.execute_reply.started":"2025-11-08T10:17:20.924831Z","shell.execute_reply":"2025-11-08T10:17:20.947272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #kaggleHub\n# import kagglehub\n# KAGGLE_USERNAME = \"shreyaaggarwal09\"  # replace\n# MODEL = \"age-gender-cnn\"\n# FRAMEWORK = \"pytorch\"\n# VARIATION = \"baseline\"\n# handle = f\"{KAGGLE_USERNAME}/{MODEL}/{FRAMEWORK}/{VARIATION}\"\n\n# kagglehub.model_upload(handle, model_path, version_notes=\"Initial baseline model trained with TrackIO logging\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:20.950195Z","iopub.execute_input":"2025-11-08T10:17:20.950825Z","iopub.status.idle":"2025-11-08T10:17:20.977468Z","shell.execute_reply.started":"2025-11-08T10:17:20.950788Z","shell.execute_reply":"2025-11-08T10:17:20.975840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# plt.figure(figsize=(8,5))\n# plt.plot(range(1, epochs+1), loss_history, marker='o', color='teal')\n# plt.title(\"Training Loss Curve\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"Loss\")\n# plt.grid(True)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset, DataLoader\n# from torchvision import transforms\n# from PIL import Image\n# import pandas as pd\n# from pathlib import Path\n\n# # Path to trained model\n# model_path = '/kaggle/input/age-gender-cnn/pytorch/baseline/1/age_gender_model.pth'\n\n# # Device\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Define your model class exactly as during training\n# # Make sure AgeGenderCnn class is defined above this cell\n# model = AgeGenderCnn().to(device)\n# model.load_state_dict(torch.load(model_path, map_location=device))\n# model.eval()\n\n# # Load test CSV\n# test_csv_path = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test.csv\"\n# test_df = pd.read_csv(test_csv_path)\n\n# # Folder where test images are stored\n# IMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test\")  \n\n# # Image transforms (same as training)\n# transform = transforms.Compose([\n#     transforms.Resize((128, 128)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n# ])\n\n# # Dataset for inference\n# class TestDataset(Dataset):\n#     def __init__(self, df, root_dir, transform=None):\n#         self.df = df\n#         self.root_dir = root_dir\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         # Convert numeric ID to filename (zero-padded)\n#         img_id = self.df.iloc[idx]['id']            # e.g., 0, 1, 2\n#         img_name = f\"{int(img_id):05d}.jpg\"         # zero-padded to 5 digits\n#         img_path = self.root_dir / img_name\n\n#         image = Image.open(img_path).convert(\"RGB\")\n#         if self.transform:\n#             image = self.transform(image)\n#         return image, idx\n\n# # DataLoader\n# test_dataset = TestDataset(test_df, root_dir=IMG_DIR, transform=transform)\n# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# # Inference loop\n# ids = []\n# pred_ages = []\n# pred_genders = []\n\n# with torch.no_grad():\n#     for xb, idx_batch in test_loader:\n#         xb = xb.to(device)\n#         pred_age, pred_gender = model(xb)\n\n#         # Convert idx_batch tensor to list of ints\n#         idx_list = idx_batch.tolist()\n#         ids.extend([test_df.iloc[i]['id'] for i in idx_list])\n\n#         pred_ages.extend(pred_age.squeeze(1).cpu().numpy())\n#         pred_genders.extend((pred_gender.squeeze(1) > 0.5).long().cpu().numpy())\n\n# # Create submission DataFrame\n# submission_df = pd.DataFrame({\n#     'id': ids,\n#     'gender': pred_genders,\n#     'age': pred_ages\n# })\n# submission_df['age'] = submission_df['age'].round().astype(int)\n\n# submission_path = '/kaggle/working/submission.csv'\n# submission_df.to_csv(submission_path, index=False)\n# print(\"  Submission file created successfully!\")\n# print(submission_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine tuned model","metadata":{}},{"cell_type":"markdown","source":"### EfficientNet","metadata":{}},{"cell_type":"code","source":"# #fine tuned\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# from torchvision import transforms\n# from PIL import Image\n# import pandas as pd\n# from pathlib import Path\n\n# # Paths\n# DATA_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset\")\n# IMG_DIR = DATA_DIR     # folder with all training images\n# CSV_PATH = DATA_DIR / \"train.csv\" # contains: image_name, gender, age\n\n# # Read CSV\n# df = pd.read_csv(CSV_PATH)\n# # Compute mean and std for age normalization\n# age_mean = df[\"age\"].mean()\n# age_std = df[\"age\"].std()\n\n# print(f\"Age mean: {age_mean:.2f}, std: {age_std:.2f}\")\n\n# # print(\"Sample data:\\n\", df.head())\n\n# # Train-validation split\n# from sklearn.model_selection import train_test_split\n# train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"gender\"], random_state=42)\n\n# #finetuned\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n#                          std=[0.229, 0.224, 0.225])\n# ])\n\n# from torch.utils.data import Dataset\n# from PIL import Image\n# import torch\n# from pathlib import Path\n\n# class FaceDataset(Dataset):\n#     def __init__(self, dataframe, root_dir, transform=None, age_mean=None, age_std=None):\n#         self.data = dataframe\n#         self.root_dir = Path(root_dir)\n#         self.transform = transform\n#         self.age_mean = age_mean\n#         self.age_std = age_std\n\n#     def __len__(self):\n#         return len(self.data)\n\n#     def __getitem__(self, idx):\n#         row = self.data.iloc[idx]\n#         img_path = self.root_dir / row[\"full_path\"]\n#         image = Image.open(img_path).convert(\"RGB\")\n\n#         gender = torch.tensor(row[\"gender\"], dtype=torch.float32)\n#         age = torch.tensor(row[\"age\"], dtype=torch.float32)\n\n#         #   Normalize age\n#         if self.age_mean is not None and self.age_std is not None:\n#             age = (age - self.age_mean) / self.age_std\n\n#         if self.transform:\n#             image = self.transform(image)\n\n#         return image, gender, age\n\n# # Create datasets & loaders\n# train_dataset = FaceDataset(train_df, IMG_DIR, transform, age_mean, age_std)\n# val_dataset   = FaceDataset(val_df, IMG_DIR, transform, age_mean, age_std)\n\n# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n# val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n\n# # Device setup\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# print(f\" Train: {len(train_dataset)} | Val: {len(val_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# from torchvision import transforms\n# from PIL import Image\n# import pandas as pd\n# from pathlib import Path\n# from torch.utils.data import Dataset, DataLoader\n\n# # Load pretrained model class (must match your training model code)\n# class AgeGenderEfficientNet(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n\n#         for param in self.backbone.parameters():\n#             param.requires_grad = False\n\n#         in_features = self.backbone.classifier[1].in_features\n#         self.backbone.classifier = nn.Identity()\n\n#         self.gender_head = nn.Linear(in_features, 1)\n#         self.age_head = nn.Linear(in_features, 1)\n\n#     def forward(self, x):\n#         feats = self.backbone(x)\n#         gender = torch.sigmoid(self.gender_head(feats))\n#         age = self.age_head(feats)\n#         return age, gender \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T13:56:54.201287Z","iopub.execute_input":"2025-11-08T13:56:54.201626Z","iopub.status.idle":"2025-11-08T13:56:54.209307Z","shell.execute_reply.started":"2025-11-08T13:56:54.201599Z","shell.execute_reply":"2025-11-08T13:56:54.208262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = AgeGenderEfficientNet().to(device)\n# criterion_gender = nn.BCELoss()      # binary gender\n# criterion_age = nn.MSELoss()         # regression age\n# optimizer = torch.optim.Adam(\n#     list(model.gender_head.parameters()) + list(model.age_head.parameters()),\n#     lr=1e-3\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:21.005491Z","iopub.execute_input":"2025-11-08T10:17:21.005802Z","iopub.status.idle":"2025-11-08T10:17:21.037429Z","shell.execute_reply.started":"2025-11-08T10:17:21.005779Z","shell.execute_reply":"2025-11-08T10:17:21.035704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loss_history = []\n\n# for epoch in range(10):\n\n#     # Unfreeze backbone after 5 epochs (optional fine-tuning)\n#     if epoch == 5:\n#         print(\"Unfreezing last block for fine-tuning...\")\n#         for param in model.backbone.features[-1].parameters():\n#             param.requires_grad = True\n#         optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\n#     model.train()\n#     running_loss = 0.0\n#     running_age_loss = 0.0\n#     running_gender_loss = 0.0\n\n#     for imgs, gender, age in train_loader:\n#         imgs = imgs.to(device)\n#         gender = gender.to(device).float()\n#         age = age.to(device).float()\n\n#         pred_age, pred_gender = model(imgs)\n\n#         pred_gender = pred_gender.squeeze(1)\n#         pred_age = pred_age.squeeze(1)\n\n#         loss_gender = criterion_gender(pred_gender, gender)\n#         loss_age = criterion_age(pred_age, age)\n\n#         loss = loss_gender + loss_age\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n#         running_age_loss += loss_age.item()\n#         running_gender_loss += loss_gender.item()\n\n#     avg_loss = running_loss / len(train_loader)\n#     avg_age_loss = running_age_loss / len(train_loader)\n#     avg_gender_loss = running_gender_loss / len(train_loader)\n\n#     loss_history.append(avg_loss)\n\n#     print(f\"Epoch [{epoch+1}/10] | Total Loss: {avg_loss:.4f} | Age Loss: {avg_age_loss:.4f} | Gender Loss: {avg_gender_loss:.4f}\")\n\n#     # TrackIO logging (safe and meaningful now)\n#     trackio.log({\n#         \"train/loss\": avg_loss,\n#         \"train/loss_age\": avg_age_loss,\n#         \"train/loss_gender\": avg_gender_loss,\n#         \"epoch\": epoch\n#     })\n# trackio.finish()\n# print(\" Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:21.038310Z","iopub.execute_input":"2025-11-08T10:17:21.038653Z","iopub.status.idle":"2025-11-08T10:17:21.066671Z","shell.execute_reply.started":"2025-11-08T10:17:21.038629Z","shell.execute_reply":"2025-11-08T10:17:21.065141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_path = Path(\"trained_model\")\n# model_path.mkdir(exist_ok=True)\n# torch.save(model.state_dict(), model_path / \"age_gender_model.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:21.067892Z","iopub.execute_input":"2025-11-08T10:17:21.068227Z","iopub.status.idle":"2025-11-08T10:17:21.098190Z","shell.execute_reply.started":"2025-11-08T10:17:21.068198Z","shell.execute_reply":"2025-11-08T10:17:21.097106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##pretrained\n# import matplotlib.pyplot as plt\n\n# plt.figure(figsize=(8, 5))\n# plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', color='teal', linewidth=2)\n# plt.title(\"Training Loss Curve\", fontsize=14)\n# plt.xlabel(\"Epoch\", fontsize=12)\n# plt.ylabel(\"Total Loss\", fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.6)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:21.127612Z","iopub.execute_input":"2025-11-08T10:17:21.127912Z","iopub.status.idle":"2025-11-08T10:17:21.157312Z","shell.execute_reply.started":"2025-11-08T10:17:21.127887Z","shell.execute_reply":"2025-11-08T10:17:21.156244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #kaggleHub\n# import kagglehub\n# KAGGLE_USERNAME = \"shreyaaggarwal09\"  # replace\n# MODEL = \"age-gender-pre-trained-cnn\"\n# FRAMEWORK = \"pytorch\"\n# VARIATION = \"baseline\"\n# handle = f\"{KAGGLE_USERNAME}/{MODEL}/{FRAMEWORK}/{VARIATION}\"\n\n# kagglehub.model_upload(handle, model_path, version_notes=\"Initial baseline model trained with TrackIO logging\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T10:17:21.159245Z","iopub.execute_input":"2025-11-08T10:17:21.159777Z","iopub.status.idle":"2025-11-08T10:17:21.204852Z","shell.execute_reply.started":"2025-11-08T10:17:21.159735Z","shell.execute_reply":"2025-11-08T10:17:21.203123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torchvision import transforms, models\n# from PIL import Image\n# import pandas as pd\n# from pathlib import Path\n# from torch.utils.data import Dataset, DataLoader\n\n# # ------------------------------\n# # Model (must match training)\n# # ------------------------------\n# class AgeGenderEfficientNet(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n\n#         for param in self.backbone.parameters():\n#             param.requires_grad = False\n\n#         in_features = self.backbone.classifier[1].in_features\n#         self.backbone.classifier = nn.Identity()\n\n#         self.gender_head = nn.Linear(in_features, 1)\n#         self.age_head = nn.Linear(in_features, 1)\n\n#     def forward(self, x):\n#         feats = self.backbone(x)\n#         gender = torch.sigmoid(self.gender_head(feats))\n#         age = self.age_head(feats)\n#         return age, gender\n\n\n# # ------------------------------\n# # Load Model\n# # ------------------------------\n# model_path = \"/kaggle/input/age-gender-pre-trained-cnn/pytorch/baseline/1/age_gender_model.pth\"\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# model = AgeGenderEfficientNet().to(device)\n\n# checkpoint = torch.load(model_path, map_location=device)\n# state_dict = model.state_dict()\n\n# # Load only matching layers\n# filtered = {k: v for k, v in checkpoint.items() if k in state_dict and v.shape == state_dict[k].shape}\n# state_dict.update(filtered)\n# model.load_state_dict(state_dict, strict=False)\n\n# model.eval()\n# print(f\"  Loaded weights for {len(filtered)}/{len(state_dict)} layers\")\n\n# # ------------------------------\n# # Load Test CSV\n# # ------------------------------\n# test_csv_path = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test.csv\"\n# test_df = pd.read_csv(test_csv_path)\n\n# IMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test\")\n\n# # ------------------------------\n# # IMPORTANT: Use SAME normalization as TRAINING\n# # ------------------------------\n# transform = transforms.Compose([\n#     transforms.Resize((128, 128)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n# ])\n\n# # ------------------------------\n# # Dataset\n# # ------------------------------\n# class TestDataset(Dataset):\n#     def __init__(self, df, root, transform):\n#         self.df = df\n#         self.root = root\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         img_id = int(self.df.iloc[idx][\"id\"])\n#         img_name = f\"{img_id:05d}.jpg\"\n#         img_path = self.root / img_name\n\n#         image = Image.open(img_path).convert(\"RGB\")\n#         image = self.transform(image)\n\n#         return image, idx\n\n# test_dataset = TestDataset(test_df, IMG_DIR, transform)\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# # ------------------------------\n# # Inference\n# # ------------------------------\n# ids, pred_genders, pred_ages = [], [], []\n\n# with torch.no_grad():\n#     for xb, idxs in test_loader:\n#         xb = xb.to(device)\n\n#         age_out, gender_out = model(xb)\n\n#         ages = age_out.squeeze(1).cpu().numpy()\n#         genders = (gender_out.squeeze(1) > 0.5).long().cpu().numpy()\n\n#         for b, df_index in enumerate(idxs.tolist()):\n#             ids.append(test_df.iloc[df_index][\"id\"])\n#             pred_ages.append(ages[b])\n#             pred_genders.append(genders[b])\n\n# # ------------------------------\n# # Create & Save Submission\n# # ------------------------------\n# submission = pd.DataFrame({\n#     \"id\": ids,\n#     \"gender\": pred_genders,\n#     \"age\": pred_ages\n# })\n\n# submission[\"age\"] = submission[\"age\"].round().astype(int)\n\n# out_path = \"/kaggle/working/submission.csv\"\n# submission.to_csv(out_path, index=False)\n\n# print(\"  Submission ready:\", out_path)\n# print(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ConvNext Tiny 1K 224 (Final Model)","metadata":{}},{"cell_type":"code","source":"!pip install trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T12:36:27.531122Z","iopub.execute_input":"2025-11-11T12:36:27.531880Z","iopub.status.idle":"2025-11-11T12:37:15.364131Z","shell.execute_reply.started":"2025-11-11T12:36:27.531852Z","shell.execute_reply":"2025-11-11T12:37:15.363425Z"}},"outputs":[{"name":"stdout","text":"Collecting trackio\n  Downloading trackio-0.8.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting gradio<6.0.0,>=5.48.0 (from gradio[oauth]<6.0.0,>=5.48.0->trackio)\n  Downloading gradio-5.49.1-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: huggingface-hub<1.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (0.36.0)\nRequirement already satisfied: numpy<3.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (1.26.4)\nRequirement already satisfied: orjson<4.0.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (3.11.0)\nRequirement already satisfied: pandas<3.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (2.2.3)\nRequirement already satisfied: pillow<12.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (11.3.0)\nCollecting plotly<7.0.0,>=6.0.0 (from trackio)\n  Downloading plotly-6.4.0-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: pydub<1.0.0 in /usr/local/lib/python3.11/dist-packages (from trackio) (0.25.1)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (4.11.0)\nRequirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (1.1.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.116.1)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.6.1)\nCollecting gradio-client==1.13.3 (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio)\n  Downloading gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.1.2)\nRequirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (3.0.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (25.0)\nCollecting pydantic<2.12,>=2.0 (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio)\n  Downloading pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (6.0.3)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.12.5)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.47.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (4.15.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.35.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.13.3->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2025.10.0)\nRequirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.13.3->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (15.0.1)\nRequirement already satisfied: authlib in /usr/local/lib/python3.11/dist-packages (from gradio[oauth]<6.0.0,>=5.48.0->trackio) (1.6.5)\nRequirement already satisfied: itsdangerous in /usr/local/lib/python3.11/dist-packages (from gradio[oauth]<6.0.0,>=5.48.0->trackio) (2.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0->trackio) (3.20.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0->trackio) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0->trackio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0->trackio) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0->trackio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0->trackio) (2025.2)\nRequirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from plotly<7.0.0,>=6.0.0->trackio) (1.48.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (3.11)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.7.0)\nCollecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio)\n  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.4.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0->trackio) (1.17.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (14.2.0)\nRequirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib->gradio[oauth]<6.0.0,>=5.48.0->trackio) (46.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0->trackio) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0->trackio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0->trackio) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0->trackio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0->trackio) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0->trackio) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0.0->trackio) (2.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0->trackio) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2.19.2)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2.0.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography->authlib->gradio[oauth]<6.0.0,>=5.48.0->trackio) (2.23)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<6.0.0,>=5.48.0->gradio[oauth]<6.0.0,>=5.48.0->trackio) (0.1.2)\nDownloading trackio-0.8.1-py3-none-any.whl (874 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.0/875.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio-5.49.1-py3-none-any.whl (63.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading plotly-6.4.0-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core, plotly, pydantic, gradio-client, gradio, trackio\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.41.5\n    Uninstalling pydantic_core-2.41.5:\n      Successfully uninstalled pydantic_core-2.41.5\n  Attempting uninstall: plotly\n    Found existing installation: plotly 5.24.1\n    Uninstalling plotly-5.24.1:\n      Successfully uninstalled plotly-5.24.1\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.12.4\n    Uninstalling pydantic-2.12.4:\n      Successfully uninstalled pydantic-2.12.4\n  Attempting uninstall: gradio-client\n    Found existing installation: gradio_client 1.11.0\n    Uninstalling gradio_client-1.11.0:\n      Successfully uninstalled gradio_client-1.11.0\n  Attempting uninstall: gradio\n    Found existing installation: gradio 5.38.1\n    Uninstalling gradio-5.38.1:\n      Successfully uninstalled gradio-5.38.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gradio-5.49.1 gradio-client-1.13.3 plotly-6.4.0 pydantic-2.11.10 pydantic-core-2.33.2 trackio-0.8.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Huggingface Access Adding token to environment variable\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nimport os\nos.environ['HF_TOKEN']  = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T12:37:17.077633Z","iopub.execute_input":"2025-11-11T12:37:17.077914Z","iopub.status.idle":"2025-11-11T12:37:17.181006Z","shell.execute_reply.started":"2025-11-11T12:37:17.077886Z","shell.execute_reply":"2025-11-11T12:37:17.180374Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import trackio\nimport torch\ntrackio.init(\n    project=\"25-t3-nppe1\", \n    space_id=\"ShreyaAgr/dlgenai-nppe\",\n    name=\"convexnet12\",\n    group=\"simple\"\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T12:37:20.099872Z","iopub.execute_input":"2025-11-11T12:37:20.100140Z","iopub.status.idle":"2025-11-11T12:37:33.826049Z","shell.execute_reply.started":"2025-11-11T12:37:20.100115Z","shell.execute_reply":"2025-11-11T12:37:33.825438Z"}},"outputs":[{"name":"stdout","text":"* Trackio project initialized: 25-t3-nppe1\n* Trackio metrics will be synced to Hugging Face Dataset: ShreyaAgr/dlgenai-nppe-dataset\n* Found existing space: https://huggingface.co/spaces/ShreyaAgr/dlgenai-nppe\n* View dashboard by going to: https://ShreyaAgr-dlgenai-nppe.hf.space/\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ShreyaAgr-dlgenai-nppe.hf.space/\" width=\"100%\" height=\"1000px\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"* Created new run: convexnet12\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================================\n#  TRAINING SCRIPT  (ConvNeXtV2-Tiny Optimized, No Age Normalization)\n# ===========================================\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import ConvNextV2Model, AutoImageProcessor\nfrom PIL import Image\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm import tqdm\n\n# ------------------------------\n# Config\n# ------------------------------\nMODEL_NAME = \"facebook/convnextv2-tiny-1k-224\"\nBATCH_SIZE = 16\nEPOCHS = 12\nLR = 1e-4\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nTRAIN_CSV = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/train.csv\"\nIMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/train\")\n\n# ------------------------------\n# Preprocessing\n# ------------------------------\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# ------------------------------\n# Dataset\n# ------------------------------\nclass FaceDataset(Dataset):\n    def __init__(self, df, root, processor):\n        self.df = df\n        self.root = root\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = self.root / f\"{int(row['id']):05d}.jpg\"\n        image = Image.open(img_path).convert(\"RGB\")\n        pixel_values = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n        age = torch.tensor(row[\"age\"], dtype=torch.float32)\n        gender = torch.tensor(row[\"gender\"], dtype=torch.float32)\n        return pixel_values, age, gender\n\n\ntrain_loader = DataLoader(\n    FaceDataset(train_df, IMG_DIR, processor),\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=2\n)\n\n# ------------------------------\n# Model\n# ------------------------------\nclass AgeGenderConvNeXt(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = ConvNextV2Model.from_pretrained(MODEL_NAME)\n        in_features = self.backbone.config.hidden_sizes[-1]\n\n        # Freeze backbone initially\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        self.gender_head = nn.Linear(in_features, 1)\n        self.age_head = nn.Linear(in_features, 1)\n\n    def forward(self, x):\n        feats = self.backbone(x).pooler_output\n        gender = torch.sigmoid(self.gender_head(feats))\n        age = self.age_head(feats)  # direct regression output (no sigmoid)\n        return age, gender\n\n\nmodel = AgeGenderConvNeXt().to(DEVICE)\n\n# ------------------------------\n# Loss & Optimizer\n# ------------------------------\ncriterion_age = nn.SmoothL1Loss()  # regression\ncriterion_gender = nn.BCELoss()    # binary classification\n\noptimizer = torch.optim.Adam(\n    list(model.gender_head.parameters()) + list(model.age_head.parameters()),\n    lr=LR\n)\n\n# ------------------------------\n# Training Loop\n# ------------------------------\nfor epoch in range(EPOCHS):\n    #   Gradual unfreezing after 2 epochs\n    if epoch == 2:\n        for p in model.backbone.encoder.stages[-1].parameters():\n            p.requires_grad = True\n        optimizer = torch.optim.Adam(model.parameters(), lr=LR / 10)\n        print(\"Unfroze last ConvNeXt block for fine-tuning\")\n\n    model.train()\n    total_age_loss, total_gender_loss = 0.0, 0.0\n\n    for xb, age, gender in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n        xb, age, gender = xb.to(DEVICE), age.to(DEVICE).unsqueeze(1), gender.to(DEVICE).unsqueeze(1)\n\n        optimizer.zero_grad()\n        pred_age, pred_gender = model(xb)\n\n        loss_age = criterion_age(pred_age, age)\n        loss_gender = criterion_gender(pred_gender, gender)\n\n        # Weighted total loss\n        loss = 0.1 * loss_age + loss_gender\n\n        loss.backward()\n        optimizer.step()\n\n        total_age_loss += loss_age.item()\n        total_gender_loss += loss_gender.item()\n\n    avg_age_loss = total_age_loss / len(train_loader)\n    avg_gender_loss = total_gender_loss / len(train_loader)\n    avg_total_loss = 0.1 * avg_age_loss + avg_gender_loss\n\n    print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n          f\"Age Loss: {avg_age_loss:.4f} | \"\n          f\"Gender Loss: {avg_gender_loss:.4f} | \"\n          f\"Total Loss: {avg_total_loss:.4f}\")\n\n#Log training progress to TrackIO\n    trackio.log({\"train/loss\": avg_total_loss, \"train/loss_age\": avg_age_loss, \"train/loss_gender\": avg_gender_loss, \"epoch\": epoch})\ntrackio.finish()\n\n# ------------------------------\n# Save Model\n# ------------------------------\nPath(\"trained_model\").mkdir(exist_ok=True)\ntorch.save(model.state_dict(), \"trained_model/age_gender_convnext.pth\")\nprint(\"  Model saved at trained_model/age_gender_convnext.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T12:37:38.514738Z","iopub.execute_input":"2025-11-11T12:37:38.515676Z","iopub.status.idle":"2025-11-11T13:30:48.504986Z","shell.execute_reply.started":"2025-11-11T12:37:38.515649Z","shell.execute_reply":"2025-11-11T13:30:48.503977Z"}},"outputs":[{"name":"stderr","text":"2025-11-11 12:37:51.356886: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762864671.749866      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762864671.889787      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06109f8b231348e6a11c046b88072a8f"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61e0501da2f486384784e8d20b24e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d929156f744bac975aa46e4b9fcfc5"}},"metadata":{}},{"name":"stderr","text":"Epoch 1/12: 100%|██████████| 2170/2170 [03:35<00:00, 10.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/12] | Age Loss: 25.1062 | Gender Loss: 0.3055 | Total Loss: 2.8161\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/12: 100%|██████████| 2170/2170 [03:32<00:00, 10.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/12] | Age Loss: 13.3153 | Gender Loss: 0.2328 | Total Loss: 1.5644\nUnfroze last ConvNeXt block for fine-tuning\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/12: 100%|██████████| 2170/2170 [04:32<00:00,  7.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/12] | Age Loss: 8.1229 | Gender Loss: 0.2065 | Total Loss: 1.0187\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/12] | Age Loss: 6.8725 | Gender Loss: 0.1758 | Total Loss: 0.8630\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/12: 100%|██████████| 2170/2170 [04:31<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/12] | Age Loss: 6.3393 | Gender Loss: 0.1564 | Total Loss: 0.7903\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/12: 100%|██████████| 2170/2170 [04:31<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/12] | Age Loss: 5.9187 | Gender Loss: 0.1402 | Total Loss: 0.7321\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/12] | Age Loss: 5.5571 | Gender Loss: 0.1267 | Total Loss: 0.6824\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/12] | Age Loss: 5.2261 | Gender Loss: 0.1137 | Total Loss: 0.6363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/12: 100%|██████████| 2170/2170 [04:31<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/12] | Age Loss: 4.8954 | Gender Loss: 0.1018 | Total Loss: 0.5914\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/12] | Age Loss: 4.5661 | Gender Loss: 0.0901 | Total Loss: 0.5467\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/12] | Age Loss: 4.2440 | Gender Loss: 0.0782 | Total Loss: 0.5026\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/12: 100%|██████████| 2170/2170 [04:31<00:00,  7.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/12] | Age Loss: 3.9227 | Gender Loss: 0.0672 | Total Loss: 0.4595\n* Run finished. Uploading logs to Trackio (please wait...)\n  Model saved at trained_model/age_gender_convnext.pth\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# # ===========================================\n# #  Upload trained PyTorch model to KaggleHub\n# # ===========================================\n\n# import kagglehub\n\n# # Define handle details\n# KAGGLE_USERNAME = \"shreyaaggarwal09\"     # your Kaggle username\n# MODEL = \"age-12gender_wn-con-trained-model\"   # model repository name\n# FRAMEWORK = \"pytorch\"\n# VARIATION = \"age_gender_convnext\"\n# handle = f\"{KAGGLE_USERNAME}/{MODEL}/{FRAMEWORK}/{VARIATION}\"\n\n# # Path to your trained model (.pth file)\n# model_path = \"trained_model/age_gender_convnext.pth\"  # <-- same as your training script\n\n# # Upload model\n# kagglehub.model_upload(\n#     handle,\n#     model_path,\n#     version_notes=\"Initial baseline model trained with TrackIO logging\"\n# )\n\n# print(\"  Model uploaded successfully to KaggleHub!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:11:32.667944Z","iopub.execute_input":"2025-11-10T14:11:32.668749Z","iopub.status.idle":"2025-11-10T14:11:36.411268Z","shell.execute_reply.started":"2025-11-10T14:11:32.668700Z","shell.execute_reply":"2025-11-10T14:11:36.410638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#inference\n\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import ConvNextV2Model, AutoImageProcessor\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\nfrom pathlib import Path\nfrom kaggle_secrets import UserSecretsClient\n\n\n# Hugging Face Authentication\nuser_secrets = UserSecretsClient()\nos.environ['HF_TOKEN'] = user_secrets.get_secret(\"hf_token\")\nprint(\" Hugging Face token loaded successfully.\")\n\n# Config\nMODEL_NAME = \"facebook/convnextv2-tiny-1k-224\"\nMODEL_PATH = \"/kaggle/input/age-12gender_wn-con-trained-model/pytorch/age_gender_convnext/1/age_gender_convnext.pth\"\nTEST_CSV = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test.csv\"\nIMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Model Definition (Same as Training)\n\nclass AgeGenderConvNeXt(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = ConvNextV2Model.from_pretrained(MODEL_NAME, token=os.environ[\"HF_TOKEN\"])\n        in_features = self.backbone.config.hidden_sizes[-1]\n\n        # Freeze backbone during inference\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        # Output heads\n        self.gender_head = nn.Linear(in_features, 1)\n        self.age_head = nn.Linear(in_features, 1)  # direct regression (no sigmoid)\n\n    def forward(self, x):\n        feats = self.backbone(x).pooler_output\n        gender = torch.sigmoid(self.gender_head(feats))  # binary classification\n        age = self.age_head(feats)                       # raw regression output\n        return age, gender\n\n# Load Trained Model Weights\nprint(\" Loading model weights...\")\nmodel = AgeGenderConvNeXt().to(DEVICE)\nmodel.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\nmodel.eval()\nprint(f\"  Model loaded successfully on {DEVICE}\")\n\n\n# Dataset + DataLoader\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME, token=os.environ[\"HF_TOKEN\"])\n\nclass TestDataset(Dataset):\n    def __init__(self, df, root, processor):\n        self.df = df\n        self.root = root\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_id = int(self.df.iloc[idx][\"id\"])\n        img_path = self.root / f\"{img_id:05d}.jpg\"\n        image = Image.open(img_path).convert(\"RGB\")\n        pixel_values = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n        return pixel_values, idx\n\n#   Load test data\ntest_df = pd.read_csv(TEST_CSV)\ntest_loader = DataLoader(\n    TestDataset(test_df, IMG_DIR, processor),\n    batch_size=64,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True\n)\n\ntorch.backends.cudnn.benchmark = True  # for speed optimization\n\n\n# Inference with tqdm Progress\nids, pred_ages, pred_genders = [], [], []\n\nprint(\"⚡ Starting inference...\")\nwith torch.no_grad():\n    for xb, idxs in tqdm(test_loader, total=len(test_loader), desc=\"🧠 Predicting\"):\n        xb = xb.to(DEVICE, non_blocking=True)\n        with torch.cuda.amp.autocast():  # mixed precision for faster inference\n            age_out, gender_out = model(xb)\n\n        # Post-processing\n        pred_age = age_out.squeeze(1).cpu().numpy()\n        pred_age = pred_age.clip(0, 100)  # limit to valid age range\n        pred_gender = (gender_out.squeeze(1) > 0.5).long().cpu().numpy()\n\n        for i, df_idx in enumerate(idxs.tolist()):\n            ids.append(test_df.iloc[df_idx][\"id\"])\n            pred_ages.append(pred_age[i])\n            pred_genders.append(pred_gender[i])\n\n        if len(ids) % 500 < len(xb):\n            print(f\"Processed {len(ids)} / {len(test_df)} images...\")\n\n\n# Save Submission\nsubmission = pd.DataFrame({\n    \"id\": ids,\n    \"gender\": pred_genders,\n    \"age\": [int(round(a)) for a in pred_ages]\n})\n\nout_path = \"/kaggle/working/submission.csv\"\nsubmission.to_csv(out_path, index=False)\n\nprint(f\"  Submission saved to: {out_path}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:35:49.351784Z","iopub.execute_input":"2025-11-10T14:35:49.352463Z","iopub.status.idle":"2025-11-10T14:36:56.754972Z","shell.execute_reply.started":"2025-11-10T14:35:49.352430Z","shell.execute_reply":"2025-11-10T14:36:56.754066Z"}},"outputs":[{"name":"stderr","text":"2025-11-10 14:36:01.819236: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762785362.012645      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762785362.065811      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":" Hugging Face token loaded successfully.\n Loading model weights...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605052085ded4a23b38840120612e707"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f80ab94345a64aa09447ab653720073b"}},"metadata":{}},{"name":"stdout","text":"  Model loaded successfully on cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7284c0482084afeb060738cfd910477"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"⚡ Starting inference...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:   0%|          | 0/136 [00:00<?, ?it/s]/tmp/ipykernel_48/3021663598.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():  # mixed precision for faster inference\n🧠 Predicting:   6%|▌         | 8/136 [00:04<00:37,  3.43it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 512 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  12%|█▏        | 16/136 [00:06<00:26,  4.48it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 1024 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  18%|█▊        | 24/136 [00:08<00:24,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 1536 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  24%|██▎       | 32/136 [00:09<00:22,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 2048 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  29%|██▉       | 40/136 [00:11<00:21,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 2560 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  35%|███▍      | 47/136 [00:13<00:19,  4.56it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 3008 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  40%|████      | 55/136 [00:14<00:17,  4.55it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 3520 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  46%|████▋     | 63/136 [00:16<00:16,  4.53it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 4032 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  52%|█████▏    | 71/136 [00:18<00:14,  4.52it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 4544 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  58%|█████▊    | 79/136 [00:20<00:12,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 5056 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  63%|██████▎   | 86/136 [00:21<00:11,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 5504 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  69%|██████▉   | 94/136 [00:23<00:09,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 6016 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  75%|███████▌  | 102/136 [00:25<00:07,  4.50it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 6528 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  81%|████████  | 110/136 [00:27<00:05,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 7040 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  87%|████████▋ | 118/136 [00:28<00:03,  4.51it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 7552 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  92%|█████████▏| 125/136 [00:30<00:02,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 8000 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting:  98%|█████████▊| 133/136 [00:32<00:00,  4.47it/s]","output_type":"stream"},{"name":"stdout","text":"Processed 8512 / 8677 images...\n","output_type":"stream"},{"name":"stderr","text":"🧠 Predicting: 100%|██████████| 136/136 [00:32<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"  Submission saved to: /kaggle/working/submission.csv\n   id  gender  age\n0   0       1   39\n1   1       0   34\n2   2       1   25\n3   3       1   31\n4   4       1   52\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/working/'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T14:37:09.465509Z","iopub.execute_input":"2025-11-10T14:37:09.466362Z","iopub.status.idle":"2025-11-10T14:37:09.471242Z","shell.execute_reply.started":"2025-11-10T14:37:09.466327Z","shell.execute_reply":"2025-11-10T14:37:09.470389Z"}},"outputs":[{"name":"stdout","text":"['submission.csv', '.virtual_documents']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Resnet50","metadata":{}},{"cell_type":"code","source":"# !pip install trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:44:00.729076Z","iopub.execute_input":"2025-11-10T08:44:00.729817Z","iopub.status.idle":"2025-11-10T08:44:32.252976Z","shell.execute_reply.started":"2025-11-10T08:44:00.729768Z","shell.execute_reply":"2025-11-10T08:44:32.252090Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Huggingface Access Adding token to environment variable\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# import os\n# os.environ['HF_TOKEN']  = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:44:34.673707Z","iopub.execute_input":"2025-11-10T08:44:34.674044Z","iopub.status.idle":"2025-11-10T08:44:34.763439Z","shell.execute_reply.started":"2025-11-10T08:44:34.674013Z","shell.execute_reply":"2025-11-10T08:44:34.762581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import trackio\n# import torch\n# trackio.init(\n#     project=\"25-t3-nppe1\", \n#     space_id=\"ShreyaAgr/dlgenai-nppe\",\n#     name=\"convexnet12\",\n#     group=\"cnn_baseline\"\n# )\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:44:36.842339Z","iopub.execute_input":"2025-11-10T08:44:36.842979Z","iopub.status.idle":"2025-11-10T08:44:43.369110Z","shell.execute_reply.started":"2025-11-10T08:44:36.842950Z","shell.execute_reply":"2025-11-10T08:44:43.368387Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ===========================================\n# #  TRAINING SCRIPT  (ResNet-50 Optimized + Fixed)\n# # ===========================================\n\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import AutoModel, AutoImageProcessor\n# from PIL import Image\n# from pathlib import Path\n# import pandas as pd\n# from tqdm import tqdm\n\n# # ------------------------------\n# # Config\n# # ------------------------------\n# MODEL_NAME = \"microsoft/resnet-50\"\n# BATCH_SIZE = 16\n# EPOCHS = 10\n# LR = 1e-4\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# TRAIN_CSV = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/train.csv\"\n# IMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/train\")\n\n# # ------------------------------\n# # Preprocessing\n# # ------------------------------\n# processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n# train_df = pd.read_csv(TRAIN_CSV)\n\n# #   Normalize age targets\n# mean_age = train_df[\"age\"].mean()\n# std_age = train_df[\"age\"].std()\n# train_df[\"age_norm\"] = (train_df[\"age\"] - mean_age) / std_age\n\n# # ------------------------------\n# # Dataset\n# # ------------------------------\n# class FaceDataset(Dataset):\n#     def __init__(self, df, root, processor):\n#         self.df = df\n#         self.root = root\n#         self.processor = processor\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         row = self.df.iloc[idx]\n#         img_path = self.root / f\"{int(row['id']):05d}.jpg\"\n#         image = Image.open(img_path).convert(\"RGB\")\n#         pixel_values = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n\n#         age = torch.tensor(row[\"age_norm\"], dtype=torch.float32)\n#         gender = torch.tensor(row[\"gender\"], dtype=torch.float32)\n#         return pixel_values, age, gender\n\n\n# train_loader = DataLoader(\n#     FaceDataset(train_df, IMG_DIR, processor),\n#     batch_size=BATCH_SIZE,\n#     shuffle=True,\n#     num_workers=2\n# )\n\n# # ------------------------------\n# # Model\n# # ------------------------------\n# class AgeGenderResNet(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n\n#         # figure out backbone output size\n#         if hasattr(self.backbone.config, \"hidden_sizes\"):\n#             in_features = self.backbone.config.hidden_sizes[-1]\n#         elif hasattr(self.backbone.config, \"hidden_size\"):\n#             in_features = self.backbone.config.hidden_size\n#         else:\n#             in_features = 2048  # Default for ResNet-50\n\n#         # Freeze backbone initially\n#         for p in self.backbone.parameters():\n#             p.requires_grad = False\n\n#         # Two heads\n#         self.gender_head = nn.Linear(in_features, 1)\n#         self.age_head = nn.Linear(in_features, 1)\n\n#     def forward(self, x):\n#         outputs = self.backbone(x)\n#         if hasattr(outputs, \"last_hidden_state\"):\n#             feats = outputs.last_hidden_state\n#         elif hasattr(outputs, \"pooler_output\"):\n#             feats = outputs.pooler_output\n#         elif isinstance(outputs, torch.Tensor):\n#             feats = outputs\n#         else:\n#             feats = outputs[0]\n\n#         # ResNet gives 4D (B, 2048, 7, 7)\n#         if feats.ndim == 4:\n#             feats = feats.mean(dim=[2, 3])  # global average pool -> (B, 2048)\n\n#         gender = torch.sigmoid(self.gender_head(feats))\n#         age = torch.sigmoid(self.age_head(feats)) * 100\n#         return age, gender\n\n\n# model = AgeGenderResNet().to(DEVICE)\n\n# # ------------------------------\n# # Loss & Optimizer\n# # ------------------------------\n# criterion_age = nn.SmoothL1Loss()\n# criterion_gender = nn.BCELoss()\n\n# optimizer = torch.optim.Adam(\n#     list(model.gender_head.parameters()) + list(model.age_head.parameters()),\n#     lr=LR\n# )\n\n# # ------------------------------\n# # Training Loop\n# # ------------------------------\n# for epoch in range(EPOCHS):\n \n#     model.train()\n#     total_age_loss, total_gender_loss = 0.0, 0.0\n\n#     for xb, age, gender in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n#         xb, age, gender = xb.to(DEVICE), age.to(DEVICE), gender.to(DEVICE).unsqueeze(1)\n\n#         optimizer.zero_grad()\n#         pred_age, pred_gender = model(xb)\n\n#         pred_age_norm = (pred_age - mean_age) / std_age\n#         loss_age = criterion_age(pred_age_norm, age)\n#         loss_gender = criterion_gender(pred_gender, gender)\n\n#         loss = 0.1 * loss_age + loss_gender\n#         loss.backward()\n#         optimizer.step()\n\n#         total_age_loss += loss_age.item()\n#         total_gender_loss += loss_gender.item()\n\n#     avg_age_loss = total_age_loss / len(train_loader)\n#     avg_gender_loss = total_gender_loss / len(train_loader)\n#     avg_total_loss = 0.1 * avg_age_loss + avg_gender_loss\n\n#     print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n#           f\"Age Loss: {avg_age_loss:.4f} | \"\n#           f\"Gender Loss: {avg_gender_loss:.4f} | \"\n#           f\"Total Loss: {avg_total_loss:.4f}\")\n\n# # ------------------------------\n# # Save Model\n# # ------------------------------\n# Path(\"trained_model\").mkdir(exist_ok=True)\n# torch.save(model.state_dict(), \"trained_model/age_gender_resnet50.pth\")\n# print(\"  Model saved at trained_model/age_gender_resnet50.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T08:44:48.304615Z","iopub.execute_input":"2025-11-10T08:44:48.305139Z","iopub.status.idle":"2025-11-10T09:07:00.026277Z","shell.execute_reply.started":"2025-11-10T08:44:48.305113Z","shell.execute_reply":"2025-11-10T09:07:00.025378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ===========================================\n# #  Upload trained PyTorch model to KaggleHub\n# # ===========================================\n\n# import kagglehub\n\n# #   Define handle details\n# KAGGLE_USERNAME = \"shreyaaggarwal09\"     # your Kaggle username\n# MODEL = \"resnet50-v2age-gender\"   # model repository name\n# FRAMEWORK = \"pytorch\"\n# VARIATION = \"baseline\"\n# handle = f\"{KAGGLE_USERNAME}/{MODEL}/{FRAMEWORK}/{VARIATION}\"\n\n# #   Path to your trained model (.pth file)\n# model_path = \"trained_model/age_gender_resnet50.pth\"  # <-- same as your training script\n\n# #   Upload model\n# kagglehub.model_upload(\n#     handle,\n#     model_path,\n#     version_notes=\"Initial baseline model trained with TrackIO logging\"\n# )\n\n# print(\"  Model uploaded successfully to KaggleHub!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:07:50.997662Z","iopub.execute_input":"2025-11-10T09:07:50.998447Z","iopub.status.idle":"2025-11-10T09:07:55.130162Z","shell.execute_reply.started":"2025-11-10T09:07:50.998418Z","shell.execute_reply":"2025-11-10T09:07:55.129543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ===========================================\n# # 🚀 FINAL INFERENCE (ResNet-50 Optimized)\n# # ===========================================\n\n# import os\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import AutoModel, AutoImageProcessor\n# from PIL import Image\n# from tqdm import tqdm\n# import pandas as pd\n# from pathlib import Path\n# from kaggle_secrets import UserSecretsClient\n\n# # ------------------------------\n# #   Hugging Face Token\n# # ------------------------------\n# user_secrets = UserSecretsClient()\n# os.environ['HF_TOKEN'] = user_secrets.get_secret(\"hf_token\")\n# print(\" Hugging Face token loaded successfully.\")\n\n# # ------------------------------\n# #   Config\n# # ------------------------------\n# MODEL_NAME = \"microsoft/resnet-50\"\n# MODEL_PATH = \"/kaggle/input/resnet50-v2age-gender/pytorch/baseline/1/age_gender_resnet50.pth\"\n# TRAIN_CSV = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/train.csv\"\n# TEST_CSV = \"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test.csv\"\n# IMG_DIR = Path(\"/kaggle/input/sep-25-dl-gen-ai-nppe-1/face_dataset/test\")\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # ------------------------------\n# #   Load Normalization Info (mean, std from training)\n# # ------------------------------\n# train_df = pd.read_csv(TRAIN_CSV)\n# mean_age = train_df[\"age\"].mean()\n# std_age = train_df[\"age\"].std()\n# print(f\"  Mean Age: {mean_age:.2f} | Std Age: {std_age:.2f}\")\n\n# # ------------------------------\n# #   Model Definition (same architecture as training)\n# # ------------------------------\n# class AgeGenderResNet(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.backbone = AutoModel.from_pretrained(MODEL_NAME, token=os.environ[\"HF_TOKEN\"])\n\n#         # Determine output size\n#         if hasattr(self.backbone.config, \"hidden_sizes\"):\n#             in_features = self.backbone.config.hidden_sizes[-1]\n#         elif hasattr(self.backbone.config, \"hidden_size\"):\n#             in_features = self.backbone.config.hidden_size\n#         else:\n#             in_features = 2048  # for ResNet-50\n\n#         # Two output heads\n#         self.gender_head = nn.Linear(in_features, 1)\n#         self.age_head = nn.Linear(in_features, 1)\n\n#     def forward(self, x):\n#         outputs = self.backbone(x)\n\n#         # Handle different backbone output formats\n#         if hasattr(outputs, \"last_hidden_state\"):\n#             feats = outputs.last_hidden_state\n#         elif hasattr(outputs, \"pooler_output\"):\n#             feats = outputs.pooler_output\n#         elif isinstance(outputs, torch.Tensor):\n#             feats = outputs\n#         else:\n#             feats = outputs[0]\n\n#         if feats.ndim == 4:\n#             feats = feats.mean(dim=[2, 3])  # GAP for ResNet\n\n#         gender = torch.sigmoid(self.gender_head(feats))\n#         age = torch.sigmoid(self.age_head(feats)) * 100  # scaled 0–100\n#         return age, gender\n\n\n# # ------------------------------\n# # Load Trained Weights\n# # ------------------------------\n# print(\"Loading trained model weights...\")\n# model = AgeGenderResNet().to(DEVICE)\n# state_dict = torch.load(MODEL_PATH, map_location=DEVICE)\n# model.load_state_dict(state_dict, strict=False)\n# model.eval()\n# print(f\" Model loaded successfully on {DEVICE}\")\n\n# # ------------------------------\n# # Dataset + DataLoader\n# # ------------------------------\n# processor = AutoImageProcessor.from_pretrained(MODEL_NAME, token=os.environ[\"HF_TOKEN\"])\n\n# class TestDataset(Dataset):\n#     def __init__(self, df, root, processor):\n#         self.df = df\n#         self.root = root\n#         self.processor = processor\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx):\n#         img_id = int(self.df.iloc[idx][\"id\"])\n#         img_path = self.root / f\"{img_id:05d}.jpg\"\n#         image = Image.open(img_path).convert(\"RGB\")\n#         pixel_values = self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n#         return pixel_values, idx\n\n\n# test_df = pd.read_csv(TEST_CSV)\n# test_loader = DataLoader(\n#     TestDataset(test_df, IMG_DIR, processor),\n#     batch_size=64,\n#     shuffle=False,\n#     num_workers=2,\n#     pin_memory=True\n# )\n\n# torch.backends.cudnn.benchmark = True  # speed optimization\n\n# # ------------------------------\n# # Inference Loop\n# # ------------------------------\n# ids, pred_ages, pred_genders = [], [], []\n# print(\"Starting inference...\")\n\n# with torch.no_grad():\n#     for xb, idxs in tqdm(test_loader, total=len(test_loader), desc=\" Predicting\"):\n#         xb = xb.to(DEVICE, non_blocking=True)\n\n#         with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n#             age_out, gender_out = model(xb)\n\n#         pred_age = age_out.squeeze(1).cpu().numpy().clip(0, 100)\n#         pred_gender = (gender_out.squeeze(1) > 0.5).long().cpu().numpy()\n\n#         for i, df_idx in enumerate(idxs.tolist()):\n#             ids.append(test_df.iloc[df_idx][\"id\"])\n#             pred_ages.append(int(round(pred_age[i])))\n#             pred_genders.append(pred_gender[i])\n\n# # ------------------------------\n# # Create Submission\n# # ------------------------------\n# submission = pd.DataFrame({\n#     \"id\": ids,\n#     \"gender\": pred_genders,\n#     \"age\": pred_ages\n# })\n\n# out_path = \"/kaggle/working/submission.csv\"\n# submission.to_csv(out_path, index=False)\n\n# print(f\"\\n  Submission saved successfully to: {out_path}\")\n# print(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T09:11:15.488688Z","iopub.execute_input":"2025-11-10T09:11:15.489613Z","iopub.status.idle":"2025-11-10T09:12:44.952884Z","shell.execute_reply.started":"2025-11-10T09:11:15.489585Z","shell.execute_reply":"2025-11-10T09:12:44.951870Z"}},"outputs":[],"execution_count":null}]}